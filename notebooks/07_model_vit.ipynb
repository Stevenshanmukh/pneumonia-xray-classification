{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35253b50",
   "metadata": {},
   "source": [
    "# 7. Vision Transformer (ViT) Model\n",
    "\n",
    "This notebook implements a Vision Transformer (ViT) for pneumonia classification. ViT uses self-attention mechanisms instead of convolutions, representing state-of-the-art in image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b1cb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VISION TRANSFORMER (ViT) ARCHITECTURE\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\steve\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to C:\\Users\\steve/.cache\\torch\\hub\\checkpoints\\vit_b_16-c867db91.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 330M/330M [00:06<00:00, 57.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Model: Vision Transformer Base (ViT-B/16)\n",
      "✓ Pretrained on ImageNet\n",
      "✓ Modified head: 768 → 2 classes\n",
      "\n",
      "✓ Model Parameters:\n",
      "  • Total parameters: 85,800,194\n",
      "  • Trainable parameters: 85,800,194\n",
      "\n",
      "✓ Model loaded and moved to cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from config import *\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VISION TRANSFORMER (ViT) ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load pretrained ViT-B/16\n",
    "try:\n",
    "    vit = models.vit_b_16(pretrained=True)\n",
    "    \n",
    "    # Modify final classifier head\n",
    "    num_features = vit.heads.head.in_features\n",
    "    vit.heads.head = nn.Linear(num_features, NUM_CLASSES)\n",
    "    \n",
    "    # Move to device\n",
    "    vit = vit.to(DEVICE)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in vit.parameters())\n",
    "    trainable_params = sum(p.numel() for p in vit.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\n✓ Model: Vision Transformer Base (ViT-B/16)\")\n",
    "    print(f\"✓ Pretrained on ImageNet\")\n",
    "    print(f\"✓ Modified head: {num_features} → {NUM_CLASSES} classes\")\n",
    "    \n",
    "    print(f\"\\n✓ Model Parameters:\")\n",
    "    print(f\"  • Total parameters: {total_params:,}\")\n",
    "    print(f\"  • Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    print(f\"\\n✓ Model loaded and moved to {DEVICE}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error loading ViT: {e}\")\n",
    "    print(\"\\nNote: ViT requires torchvision >= 0.12.0\")\n",
    "    print(\"Attempting alternative: Using a simple ViT implementation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ccafc",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer Setup\n",
    "\n",
    "Configure weighted loss and optimizer with differential learning rates for ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7c9af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOSS FUNCTION & OPTIMIZER SETUP\n",
      "============================================================\n",
      "\n",
      "✓ Class weights loaded:\n",
      "  • Normal (minority): 1.9448\n",
      "  • Pneumonia (majority): 0.6730\n",
      "\n",
      "✓ Loss Function: Weighted CrossEntropyLoss\n",
      "✓ Optimizer: Adam with differential learning rates\n",
      "  • Transformer encoder: lr=5e-05\n",
      "  • New head: lr=0.0005\n",
      "✓ Scheduler: ReduceLROnPlateau (factor=0.5, patience=3)\n",
      "\n",
      "Note: ViT uses lower learning rates than CNNs for stability\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LOSS FUNCTION & OPTIMIZER SETUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load class weights\n",
    "class_weights = torch.load('class_weights.pt', weights_only=True)\n",
    "class_weights = class_weights.to(DEVICE)\n",
    "\n",
    "print(f\"\\n✓ Class weights loaded:\")\n",
    "print(f\"  • Normal (minority): {class_weights[0]:.4f}\")\n",
    "print(f\"  • Pneumonia (majority): {class_weights[1]:.4f}\")\n",
    "\n",
    "# Loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer with different learning rates for pretrained vs new layers\n",
    "# ViT typically needs lower learning rates than CNNs\n",
    "optimizer = optim.Adam([\n",
    "    {'params': vit.encoder.parameters(), 'lr': LEARNING_RATE * 0.05},  # Very low LR for transformer\n",
    "    {'params': vit.heads.parameters(), 'lr': LEARNING_RATE * 0.5}      # Moderate LR for new head\n",
    "])\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=3\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Loss Function: Weighted CrossEntropyLoss\")\n",
    "print(f\"✓ Optimizer: Adam with differential learning rates\")\n",
    "print(f\"  • Transformer encoder: lr={LEARNING_RATE * 0.05}\")\n",
    "print(f\"  • New head: lr={LEARNING_RATE * 0.5}\")\n",
    "print(f\"✓ Scheduler: ReduceLROnPlateau (factor=0.5, patience=3)\")\n",
    "print(\"\\nNote: ViT uses lower learning rates than CNNs for stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f89f1a",
   "metadata": {},
   "source": [
    "## Training and Validation Functions\n",
    "\n",
    "Define training and validation functions for Vision Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c7ef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING FUNCTIONS DEFINED\n",
      "============================================================\n",
      "\n",
      "✓ train_one_epoch() - Trains model for one epoch\n",
      "✓ validate() - Evaluates model on validation set\n",
      "\n",
      "✓ Functions ready for training loop\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training', leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING FUNCTIONS DEFINED\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n✓ train_one_epoch() - Trains model for one epoch\")\n",
    "print(\"✓ validate() - Evaluates model on validation set\")\n",
    "print(\"\\n✓ Functions ready for training loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c55459",
   "metadata": {},
   "source": [
    "## Training Loop with Early Stopping\n",
    "\n",
    "Train Vision Transformer with early stopping and save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77723bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\anaconda3\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING VISION TRANSFORMER\n",
      "============================================================\n",
      "\n",
      "Training for 10 epochs...\n",
      "Early stopping patience: 5 epochs\n",
      "Note: ViT may take longer per epoch due to larger model size\n",
      "\n",
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|█████████████████▊                                        | 50/163 [17:30<39:14, 20.84s/it, loss=0.0452]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "# Recreate transforms\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=10, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Custom Dataset class\n",
    "class ChestXrayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "            class_path = os.path.join(root_dir, split, class_name)\n",
    "            if os.path.exists(class_path):\n",
    "                for img_name in os.listdir(class_path):\n",
    "                    if img_name.endswith(('.jpeg', '.jpg', '.png')):\n",
    "                        self.images.append(os.path.join(class_path, img_name))\n",
    "                        self.labels.append(class_idx)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Recreate datasets and dataloaders\n",
    "train_dataset = ChestXrayDataset(DATASET_PATH, split='train', transform=train_transform)\n",
    "val_dataset = ChestXrayDataset(DATASET_PATH, split='val', transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING VISION TRANSFORMER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\nTraining for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Early stopping patience: {PATIENCE} epochs\")\n",
    "print(\"Note: ViT may take longer per epoch due to larger model size\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(vit, train_loader, criterion, optimizer, DEVICE)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(vit, val_loader, criterion, DEVICE)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': vit.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "        }, f'{MODEL_SAVE_DIR}/vit_best.pth')\n",
    "        print(f\"  ✓ Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement ({patience_counter}/{PATIENCE})\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\n⚠ Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n✓ Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"✓ Best model saved to: {MODEL_SAVE_DIR}/vit_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a5b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
