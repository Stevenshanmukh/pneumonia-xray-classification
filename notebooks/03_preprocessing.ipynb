{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c875544a",
   "metadata": {},
   "source": [
    "# 3. Preprocessing & Augmentation Pipeline\n",
    "\n",
    "This notebook creates the data preprocessing pipeline with medical-appropriate augmentations using Albumentations, builds PyTorch Dataset and DataLoader classes, and computes class weights for handling imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a42261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AUGMENTATION PIPELINE SETUP\n",
      "============================================================\n",
      "\n",
      "✓ Training Transforms:\n",
      "  • Resize to 224x224\n",
      "  • Horizontal Flip (p=0.5)\n",
      "  • Rotation ±10° (p=0.5)\n",
      "  • Shift/Scale/Rotate (p=0.5)\n",
      "  • Brightness/Contrast adjustment (p=0.5)\n",
      "  • Gaussian Noise (p=0.3)\n",
      "  • ImageNet Normalization\n",
      "\n",
      "✓ Validation/Test Transforms:\n",
      "  • Resize to 224x224\n",
      "  • ImageNet Normalization only\n",
      "\n",
      "✓ Augmentation pipeline created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\anaconda3\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_8352\\117308263.py:25: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(10.0, 30.0), p=0.3),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from config import DATASET_PATH, CLASS_NAMES, IMAGE_SIZE, BATCH_SIZE, NUM_WORKERS, RANDOM_SEED\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AUGMENTATION PIPELINE SETUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Training augmentations - medical imaging appropriate\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=10, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "    A.GaussNoise(var_limit=(10.0, 30.0), p=0.3),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Validation/Test augmentations - no data augmentation\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"\\n✓ Training Transforms:\")\n",
    "print(\"  • Resize to 224x224\")\n",
    "print(\"  • Horizontal Flip (p=0.5)\")\n",
    "print(\"  • Rotation ±10° (p=0.5)\")\n",
    "print(\"  • Shift/Scale/Rotate (p=0.5)\")\n",
    "print(\"  • Brightness/Contrast adjustment (p=0.5)\")\n",
    "print(\"  • Gaussian Noise (p=0.3)\")\n",
    "print(\"  • ImageNet Normalization\")\n",
    "\n",
    "print(\"\\n✓ Validation/Test Transforms:\")\n",
    "print(\"  • Resize to 224x224\")\n",
    "print(\"  • ImageNet Normalization only\")\n",
    "\n",
    "print(\"\\n✓ Augmentation pipeline created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c871d",
   "metadata": {},
   "source": [
    "## PyTorch Dataset Class\n",
    "\n",
    "Create a custom PyTorch Dataset class for loading and transforming chest X-ray images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4b4a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET CREATION\n",
      "============================================================\n",
      "\n",
      "TRAIN - Loaded 5216 images\n",
      "VAL - Loaded 47 images\n",
      "TEST - Loaded 624 images\n",
      "\n",
      "✓ All datasets created successfully\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset class\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load image paths and labels\n",
    "        for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "            class_path = os.path.join(root_dir, split, class_name)\n",
    "            if os.path.exists(class_path):\n",
    "                for img_name in os.listdir(class_path):\n",
    "                    if img_name.endswith(('.jpeg', '.jpg', '.png')):\n",
    "                        self.images.append(os.path.join(class_path, img_name))\n",
    "                        self.labels.append(class_idx)\n",
    "        \n",
    "        print(f\"{split.upper()} - Loaded {len(self.images)} images\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET CREATION\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ChestXrayDataset(DATASET_PATH, split='train', transform=train_transform)\n",
    "val_dataset = ChestXrayDataset(DATASET_PATH, split='val', transform=val_transform)\n",
    "test_dataset = ChestXrayDataset(DATASET_PATH, split='test', transform=val_transform)\n",
    "\n",
    "print(\"\\n✓ All datasets created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1601b1",
   "metadata": {},
   "source": [
    "## Class Weights Calculation\n",
    "\n",
    "Calculate class weights to handle the imbalanced dataset during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0197f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLASS WEIGHTS CALCULATION\n",
      "============================================================\n",
      "\n",
      "Class counts in training set:\n",
      "  • Normal (class 0): 1341 (25.7%)\n",
      "  • Pneumonia (class 1): 3875 (74.3%)\n",
      "\n",
      "Class weights (for loss function):\n",
      "  • Normal (minority): 1.9448 (HIGHER weight)\n",
      "  • Pneumonia (majority): 0.6730 (LOWER weight)\n",
      "\n",
      "Imbalance ratio: 2.89:1 (Pneumonia:Normal)\n",
      "Weight ratio: 2.89:1 (Normal:Pneumonia)\n",
      "\n",
      "✓ Class weights saved to 'class_weights.pt'\n",
      "✓ Minority class (Normal) receives ~2.89x higher weight\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CLASS WEIGHTS CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate class weights for imbalance\n",
    "train_labels = train_dataset.labels\n",
    "class_counts = np.bincount(train_labels)\n",
    "\n",
    "# Calculate weights: inverse of frequency\n",
    "# Higher weight for minority class (Normal)\n",
    "total_samples = len(train_labels)\n",
    "class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "class_weights_tensor = torch.FloatTensor(class_weights)\n",
    "\n",
    "print(f\"\\nClass counts in training set:\")\n",
    "print(f\"  • Normal (class 0): {class_counts[0]} ({class_counts[0]/total_samples*100:.1f}%)\")\n",
    "print(f\"  • Pneumonia (class 1): {class_counts[1]} ({class_counts[1]/total_samples*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nClass weights (for loss function):\")\n",
    "print(f\"  • Normal (minority): {class_weights[0]:.4f} (HIGHER weight)\")\n",
    "print(f\"  • Pneumonia (majority): {class_weights[1]:.4f} (LOWER weight)\")\n",
    "\n",
    "print(f\"\\nImbalance ratio: {class_counts[1]/class_counts[0]:.2f}:1 (Pneumonia:Normal)\")\n",
    "print(f\"Weight ratio: {class_weights[0]/class_weights[1]:.2f}:1 (Normal:Pneumonia)\")\n",
    "\n",
    "# Save class weights for model training\n",
    "torch.save(class_weights_tensor, 'class_weights.pt')\n",
    "print(\"\\n✓ Class weights saved to 'class_weights.pt'\")\n",
    "print(\"✓ Minority class (Normal) receives ~2.89x higher weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc4c04",
   "metadata": {},
   "source": [
    "## DataLoader Creation\n",
    "\n",
    "Create PyTorch DataLoaders for training, validation, and test sets with appropriate batching and shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15ad450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATALOADER CREATION\n",
      "============================================================\n",
      "\n",
      "✓ Train DataLoader:\n",
      "  • Batch size: 32\n",
      "  • Number of batches: 163\n",
      "  • Shuffle: True\n",
      "\n",
      "✓ Validation DataLoader:\n",
      "  • Batch size: 32\n",
      "  • Number of batches: 2\n",
      "  • Shuffle: False\n",
      "\n",
      "✓ Test DataLoader:\n",
      "  • Batch size: 32\n",
      "  • Number of batches: 20\n",
      "  • Shuffle: False\n",
      "\n",
      "✓ All DataLoaders created successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATALOADER CREATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Train DataLoader:\")\n",
    "print(f\"  • Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  • Number of batches: {len(train_loader)}\")\n",
    "print(f\"  • Shuffle: True\")\n",
    "\n",
    "print(f\"\\n✓ Validation DataLoader:\")\n",
    "print(f\"  • Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  • Number of batches: {len(val_loader)}\")\n",
    "print(f\"  • Shuffle: False\")\n",
    "\n",
    "print(f\"\\n✓ Test DataLoader:\")\n",
    "print(f\"  • Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  • Number of batches: {len(test_loader)}\")\n",
    "print(f\"  • Shuffle: False\")\n",
    "\n",
    "print(\"\\n✓ All DataLoaders created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f762d3",
   "metadata": {},
   "source": [
    "## Test Data Pipeline\n",
    "\n",
    "Verify the data pipeline by loading a sample batch and checking tensor shapes and properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2daad066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING DATA PIPELINE\n",
      "============================================================\n",
      "\n",
      "⏳ Loading sample batch...\n",
      "\n",
      "✓ Sample batch loaded successfully\n",
      "\n",
      "Batch properties:\n",
      "  • Images shape: torch.Size([32, 3, 224, 224])\n",
      "  • Labels shape: torch.Size([32])\n",
      "  • Image dtype: torch.float32\n",
      "  • Label dtype: torch.int64\n",
      "  • Image value range: [-2.118, 2.640]\n",
      "  • Unique labels in batch: [0, 1]\n",
      "\n",
      "✓ Expected shape: (32, 3, 224, 224)\n",
      "✓ Actual shape: torch.Size([32, 3, 224, 224])\n",
      "✓ Image dimensions correct!\n",
      "\n",
      "============================================================\n",
      "NOTEBOOK 3 COMPLETE - preprocessing.ipynb\n",
      "============================================================\n",
      "Next: Create 'model_baseline_cnn.ipynb' for baseline model\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TESTING DATA PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a test loader with num_workers=0 to avoid Windows multiprocessing issues\n",
    "test_train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0,  # Set to 0 for testing\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(\"\\n⏳ Loading sample batch...\")\n",
    "\n",
    "# Test data loading\n",
    "sample_batch = next(iter(test_train_loader))\n",
    "images, labels = sample_batch\n",
    "\n",
    "print(f\"\\n✓ Sample batch loaded successfully\")\n",
    "print(f\"\\nBatch properties:\")\n",
    "print(f\"  • Images shape: {images.shape}\")\n",
    "print(f\"  • Labels shape: {labels.shape}\")\n",
    "print(f\"  • Image dtype: {images.dtype}\")\n",
    "print(f\"  • Label dtype: {labels.dtype}\")\n",
    "print(f\"  • Image value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"  • Unique labels in batch: {labels.unique().tolist()}\")\n",
    "\n",
    "# Verify dimensions\n",
    "expected_shape = (BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "print(f\"\\n✓ Expected shape: {expected_shape}\")\n",
    "print(f\"✓ Actual shape: {images.shape}\")\n",
    "\n",
    "if images.shape[1:] == expected_shape[1:]:\n",
    "    print(\"✓ Image dimensions correct!\")\n",
    "else:\n",
    "    print(\"✗ Image dimensions mismatch!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTEBOOK 3 COMPLETE - preprocessing.ipynb\")\n",
    "print(\"=\"*60)\n",
    "print(\"Next: Create 'model_baseline_cnn.ipynb' for baseline model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5869982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
